{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluation of text retrievals systems\n",
    "\n",
    "Usage of the Cranfield methodology like in data science. With precision and recall at a given cutoff length.\n",
    "\n",
    "* __Precision (P)__ = |relevant documents retrieved|/|total retrieved documents|, aka __are the retrieved results all relevant?__\n",
    "* __Recall (R)__ = |relevant documents retrieved|/|total relevant documents|, aka __have all the relevant documents been retrieved?__\n",
    "* Fb-measure = ((b^2 + 1) * PR) / (b^2 * P + R), combines P and R via geometric mean.\n",
    "\n",
    "### How to Summarize a Ranking\n",
    "\n",
    "#### Average precision\n",
    "\n",
    "Average precision is the sum of increased P divided by the number of relevant documents in collection. Using binary relevance judgement (0: not relevant, 1: relevant).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_precision(ranked_list, total_relevant_doc):\n",
    "    sum_precision = 0\n",
    "    current_relevant_doc = 0\n",
    "    for index, doc_relevance in enumerate(ranked_list):\n",
    "        if doc_relevance != 0:\n",
    "            current_relevant_doc += 1\n",
    "            sum_precision += current_relevant_doc / (index + 1)\n",
    "    average_precision = sum_precision / total_relevant_doc\n",
    "    return average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_list_A = [1,1,0,0,0,0,0,0,0,0]\n",
    "average_precision(ranked_list_A, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_list_B = [0,1,0,0,1,0,0,0,0,1]\n",
    "average_precision(ranked_list_B, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Discounted Cumulative Gain (nDCG)\n",
    "\n",
    "Using multi level relevance judgement (1: not relevant to N: most relevant, with N > 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dcg(ranked_list, max_relevance=3):\n",
    "    dcg_value = 0\n",
    "    for index, doc_relevance in enumerate(ranked_list):\n",
    "        if index == 0:\n",
    "            dcg_value += doc_relevance\n",
    "        else:\n",
    "            dcg_value += doc_relevance / log(index + 1)\n",
    "    return dcg_value\n",
    "\n",
    "def ndcg(ranked_list, max_relevance=3):\n",
    "    optimal_dcg = dcg([max_relevance] * len(ranked_list), max_relevance)\n",
    "    dcg_value = dcg(ranked_list)\n",
    "    ndcg_value = dcg_value / optimal_dcg\n",
    "    return ndcg_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGC: 11.869906908688476\n",
      "nDGC: 0.5902216528493285\n"
     ]
    }
   ],
   "source": [
    "ranked_list = [3,2,1,1,3,1,1,2,1]\n",
    "print('DGC: {}'.format(dcg(ranked_list)))\n",
    "print('nDGC: {}'.format(ndcg(ranked_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
